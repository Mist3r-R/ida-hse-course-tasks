{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Майнор \"Прикладные задачи анализа данных\"\n",
    "## Предсказание цены акции по экономическим новостям\n",
    "\n",
    "---\n",
    "### Задания:\n",
    "1. Предварительная обработка текстов и эксплоративный анализ\n",
    "2. Baseline алгоритм\n",
    "3. Творческая часть\n",
    "\n",
    "Входные данные:\n",
    "* Новости о компании \"Газпром\", начиная с 2010 года\n",
    "* Стоимость акций компании \"Газпром\" на ММВБ, начиная с 2010 года\n",
    "    * цена открытия (Open)\n",
    "    * цена закрытия (ClosingPrice)\n",
    "    * максимальная цена за день (DailyHigh)\n",
    "    * минимальная цена за день (DailyLow) \n",
    "    * объем бумаг (VolumePcs)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "#remove comment to download\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.stats.stats import pearsonr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Функция для загрузки таблиц. Также нам пригодится список дат, которые есть в обоих таблицах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    txts = pd.read_csv('data/texts.csv', parse_dates=[0])\n",
    "    prices = pd.read_csv('data/gazprom_prices.csv', sep=';', parse_dates=[0])\n",
    "    prices.sort_values('Date', inplace=True)\n",
    "    txts.sort_values('date', inplace=True)\n",
    "    prices.set_index('Date', inplace=True)\n",
    "    prices['ClosingPrice'] = pd.to_numeric(prices['ClosingPrice'].str.replace(\",\",\".\"))\n",
    "    txts.set_index('date', inplace=True)\n",
    "    news_idx = txts.index.unique()\n",
    "    pr_idx = prices.index.unique()\n",
    "    idx = list(set(pr_idx).intersection(news_idx))\n",
    "    \n",
    "    return txts, prices, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Вводная [3 балла]\n",
    "\n",
    "Проведите предобработку текстов: если считаете нужным, выполните токенизацию, приведение к нижнему регистру, лемматизацию и/или стемминг. Ответьте на следующие вопросы:\n",
    "* Есть ли корреляция между средней длинной текста за день и ценой закрытия?\n",
    "* Есть ли корреляция между количеством упоминаний Алексея Миллера  и ценой закрытия? Учтите разные варианты написания имени.\n",
    "* Упоминаний какого газопровода в статьях больше: \n",
    "    * \"северный поток\"\n",
    "    * \"турецкий поток\"?\n",
    "* Кого упоминают чаще:\n",
    "    * Алексея Миллера\n",
    "    * Владимира Путина?\n",
    "* О каких санкциях пишут в статьях?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, pr_all, idx = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>\"Газпром\" не исключает в 2010 г. выпуска обли...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-19</th>\n",
       "      <td>\"Газпром\" готов забирать весь объем азербайдж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-28</th>\n",
       "      <td>Консорциум во главе с российским ОАО \"Газпром...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-07</th>\n",
       "      <td>Газпромбанк открыл на Кипре дочернюю компанию...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text\n",
       "date                                                         \n",
       "2010-01-02   \"Газпром\" не исключает в 2010 г. выпуска обли...\n",
       "2010-01-19   \"Газпром\" готов забирать весь объем азербайдж...\n",
       "2010-01-28   Консорциум во главе с российским ОАО \"Газпром...\n",
       "2010-02-07   Газпромбанк открыл на Кипре дочернюю компанию..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>ClosingPrice</th>\n",
       "      <th>DailyHigh</th>\n",
       "      <th>DailyLow</th>\n",
       "      <th>VolumePcs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>184,74000</td>\n",
       "      <td>189.85</td>\n",
       "      <td>190,40000</td>\n",
       "      <td>183,50000</td>\n",
       "      <td>76298175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>168,70000</td>\n",
       "      <td>168.20</td>\n",
       "      <td>170,71000</td>\n",
       "      <td>166,33000</td>\n",
       "      <td>58570262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>172,49000</td>\n",
       "      <td>175.00</td>\n",
       "      <td>176,14000</td>\n",
       "      <td>172,33000</td>\n",
       "      <td>94994135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>158,20000</td>\n",
       "      <td>159.26</td>\n",
       "      <td>160,31000</td>\n",
       "      <td>154,39000</td>\n",
       "      <td>67031024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open  ClosingPrice  DailyHigh   DailyLow  VolumePcs\n",
       "Date                                                                \n",
       "2010-01-02  184,74000        189.85  190,40000  183,50000   76298175\n",
       "2010-01-03  168,70000        168.20  170,71000  166,33000   58570262\n",
       "2010-01-04  172,49000        175.00  176,14000  172,33000   94994135\n",
       "2010-01-06  158,20000        159.26  160,31000  154,39000   67031024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_all.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизация\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "all_text = []\n",
    "prog = re.compile('[а-яё-]+')\n",
    "for i in range(len(df)):\n",
    "    s = df['text'][i]\n",
    "    df['text'].iloc[i] = prog.findall(s.lower())\n",
    "    for j in range(len(df['text'][i])):\n",
    "        all_text.append(df['text'][i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция создания всех вариаций падежей для конкретного слова\n",
    "\n",
    "case = ['nomn', 'gent', 'datv', 'accs', 'ablt', 'loct']\n",
    "def cases(word):\n",
    "    changed_word = []\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    parsed_word = morph.parse(word)[0]\n",
    "    for i in range(len(case)):\n",
    "        changed_word.append(parsed_word.inflect({case[i]}).word)\n",
    "    return changed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# корреляция длины текста и цены закрытия\n",
    "# корреляция длины Александра Миллера и цены закрытия\n",
    "\n",
    "lens = []\n",
    "prices = []\n",
    "countes = []\n",
    "cases_miller = cases('миллер')\n",
    "        \n",
    "texts = df[df.index.isin(idx)]\n",
    "prs = pr_all[pr_all.index.isin(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(idx)):\n",
    "    lens.append(len(texts.loc[texts.index[i],'text']))\n",
    "    temp = prs.loc[prs.index[i],'ClosingPrice']\n",
    "    prices.append(temp) \n",
    "    count = 0\n",
    "    freqDist = nltk.FreqDist(texts.loc[texts.index[i], 'text'])\n",
    "    for j in range(len(cases_miller)):\n",
    "        count += freqDist[cases_miller[j]]\n",
    "    countes.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.012405743888311147, 0.67309599859248292)\n",
      "(0.0055160956175063303, 0.8511986601879219)\n"
     ]
    }
   ],
   "source": [
    "print(pearsonr(lens, prices))\n",
    "print(pearsonr(countes, prices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициент корреляции для связи длинны текста с ценой закрытия равен 0.012\n",
    "\n",
    "Коэффициент корреляции для связи Александра Миллера с ценой закрытия равен 0.005\n",
    "\n",
    "Как мы видим - корреляция очень маленькая => связь между величинами не берем в расчет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Турецкий поток: 61 \n",
      "Северный поток: 36\n",
      "Владимир Путин: 101 \n",
      "Александр Миллер: 176\n"
     ]
    }
   ],
   "source": [
    "# Сравнение потоков и сравнение Путина и Миллера\n",
    "\n",
    "cases_nord = cases('северный')\n",
    "cases_turkey = cases('турецкий')\n",
    "cases_putin = cases('путин')\n",
    "cases_miller = cases('миллер')\n",
    "\n",
    "freqDist = nltk.FreqDist(all_text)\n",
    "\n",
    "count_nord = 0\n",
    "count_turkey = 0\n",
    "count_putin = 0\n",
    "count_miller = 0\n",
    "\n",
    "for j in range(len(cases_nord)):\n",
    "    count_nord += freqDist[cases_nord[j]]\n",
    "    count_turkey += freqDist[cases_turkey[j]]\n",
    "    count_putin += freqDist[cases_putin[j]]\n",
    "    count_miller += freqDist[cases_miller[j]]\n",
    "    \n",
    "print('Турецкий поток:', count_turkey, \"\\nСеверный поток:\", count_nord)\n",
    "print('Владимир Путин:', count_putin, \"\\nАлександр Миллер:\", count_miller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Санкции\n",
    "\n",
    "df2 = pd.read_csv('data/texts.csv')\n",
    "count = 0\n",
    "cases_sanction = cases('санкция')\n",
    "sanctions = []\n",
    "\n",
    "for i in range(len(df['text'])):\n",
    "    for j in range(len(cases_sanction)):\n",
    "        index = df2['text'][i].find(cases_sanction[j])\n",
    "        \n",
    "        new = ''\n",
    "        if index != -1:\n",
    "            index -= 2\n",
    "            while (df2['text'][i][index] != ' '):\n",
    "                new += df2['text'][i][index]\n",
    "                index -= 1\n",
    "                new1 = new[::-1]\n",
    "            sanctions.append(new1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['банкам',\n",
       " 'под',\n",
       " 'осложняют',\n",
       " 'под',\n",
       " 'и',\n",
       " 'западные',\n",
       " 'экономическими',\n",
       " 'антироссийскими',\n",
       " 'персональные',\n",
       " 'на',\n",
       " 'затронутых',\n",
       " 'штрафные',\n",
       " 'числе',\n",
       " 'что',\n",
       " 'под',\n",
       " 'западные',\n",
       " 'запрета,',\n",
       " 'под',\n",
       " 'финансовые',\n",
       " 'ввести',\n",
       " 'ждут',\n",
       " 'международные']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что санкции могут быть:\n",
    "    Западные (x2),\n",
    "    Экономические (x1),\n",
    "    Антироссийские (x1),\n",
    "    Персональные (x1),\n",
    "    Штрафные (х1),\n",
    "    Затронутые (x1),\n",
    "    Финансовые (x1),\n",
    "    Международные (x1).\n",
    "    \n",
    "Чаше всего пишут про западные санкции.\n",
    "\n",
    "(2 метод выполнения этого пункта представлен ниже)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 22 of 22 matches:\n",
      "тране подпадающей под международные санкции на кубе россия которая не хочет тер\n",
      "тране подпадающей под международные санкции на кубе газпром-медиа нашел покупат\n",
      "адельца геннадия тимченко попал под санкции сша может в будущем выйти из черног\n",
      "о уже несколько лет пытается ввести санкции к компаниям и их топ-менеджерам за \n",
      "т подпасть под визовые и финансовые санкции которые европа планирует ввести в о\n",
      "енеджеров но после анализа ситуации санкции могут распространиться и на деятель\n",
      "дному из проектов сланцевой нефти с санкции могут затронуть и другое сп общей т\n",
      "дному из проектов сланцевой нефти с санкции могут затронуть и другое сп общей т\n",
      "ржит госбанки попавшие под западные санкции в список банков которые получат воз\n",
      "мотря на отсутствие прямого запрета санкции могут привести к трудностям с поста\n",
      "транить наши опасения компанию ждут санкции пока еврокомиссия получила хорошие \n",
      "ашим финансовым институтам развития санкции - объяснил министр ведет переговоры\n",
      "порте газпромбанк гпб подпавший под санкции сша и ес полностью погасил трехлетн\n",
      " в договоры с подрядчиками штрафные санкции за несогласованные публикации о зак\n",
      "по мсфо в газпром нефти уверяют что санкции никак не влияют на ее деятельность \n",
      "млрд руб украина может использовать санкции против газпрома для радикального пе\n",
      "налу россия- газпром не ожидает что санкции окажут значительное влияние на фина\n",
      " востоке страны газпром несмотря на санкции и проблемы с привлечением финансиро\n",
      "газпром который до сих пор западные санкции почти не затрагивали впервые попал \n",
      "нк так и подпавшие под персональные санкции рнкб смп банк и банк россия а также\n",
      "этом направлении серьезно осложняют санкции сша газпром закладывает в г млрд ру\n",
      "инского месторождения попавшего под санкции по мнению экспертов пока речь может\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    "textList = Text(all_text)\n",
    "textList.concordance('санкции')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Классификационная [3 балла]\n",
    "Вам предстоит решить следующую задачу: по текстам новостей за день определить, вырастет или понизится цена закрытия.\n",
    "Для этого:\n",
    "* бинаризуйте признак \"цена закрытия\":  новый признак ClosingPrice_bin равен 1, если по сравнению со вчера цена не упала, и 0 – в обратном случаея;\n",
    "* составьте бучающее и тестовое множество: данные до начала 2016 года используются для обучения, данные с 2016 года и позже – для тестирования.\n",
    "\n",
    "Таким образом, в каждлый момент времени мы знаем: \n",
    "* ClosingPrice_bin – бинарый целевой признак\n",
    "* слова из статей, опубликованных в этот день – объясняющие признаки\n",
    "\n",
    "В этой части задания вам нужно сделать baseline алгоритм и попытаться его улучшить в следующей части. \n",
    "\n",
    "Используйте любой известный вам алгоритм классификации текстов для того, Используйте $tf-idf$ преобразование, сингулярное разложение, нормировку признакого пространства и любые другие техники обработки данных, которые вы считаете нужным. Используйте accuracy и F-measure для оценки качества классификации. Покажите, как  $tf-idf$ преобразование или сингулярное разложение или любая другая использованная вами техника влияет на качество классификации.\n",
    "Если у выбранного вами алгоритма есть гиперпараметры (например, $\\alpha$ в преобразовании Лапласа для метода наивного Байеса), покажите, как изменение гиперпараметра влияет на качество классификации.\n",
    "\n",
    "---\n",
    "\n",
    "Загрузим данные в датафреймы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>\"Газпром\" не исключает в 2010 г. выпуска обли...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-19</th>\n",
       "      <td>\"Газпром\" готов забирать весь объем азербайдж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-28</th>\n",
       "      <td>Консорциум во главе с российским ОАО \"Газпром...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-07</th>\n",
       "      <td>Газпромбанк открыл на Кипре дочернюю компанию...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-09</th>\n",
       "      <td>\"Газпром\" вновь понизил прогноз экспорта в Ев...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text\n",
       "date                                                         \n",
       "2010-01-02   \"Газпром\" не исключает в 2010 г. выпуска обли...\n",
       "2010-01-19   \"Газпром\" готов забирать весь объем азербайдж...\n",
       "2010-01-28   Консорциум во главе с российским ОАО \"Газпром...\n",
       "2010-02-07   Газпромбанк открыл на Кипре дочернюю компанию...\n",
       "2010-02-09   \"Газпром\" вновь понизил прогноз экспорта в Ев..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts, prices, idx = load_data()\n",
    "txts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>ClosingPrice</th>\n",
       "      <th>DailyHigh</th>\n",
       "      <th>DailyLow</th>\n",
       "      <th>VolumePcs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>184,74000</td>\n",
       "      <td>189.85</td>\n",
       "      <td>190,40000</td>\n",
       "      <td>183,50000</td>\n",
       "      <td>76298175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>168,70000</td>\n",
       "      <td>168.20</td>\n",
       "      <td>170,71000</td>\n",
       "      <td>166,33000</td>\n",
       "      <td>58570262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>172,49000</td>\n",
       "      <td>175.00</td>\n",
       "      <td>176,14000</td>\n",
       "      <td>172,33000</td>\n",
       "      <td>94994135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>158,20000</td>\n",
       "      <td>159.26</td>\n",
       "      <td>160,31000</td>\n",
       "      <td>154,39000</td>\n",
       "      <td>67031024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open  ClosingPrice  DailyHigh   DailyLow  VolumePcs\n",
       "Date                                                                \n",
       "2010-01-02  184,74000        189.85  190,40000  183,50000   76298175\n",
       "2010-01-03  168,70000        168.20  170,71000  166,33000   58570262\n",
       "2010-01-04  172,49000        175.00  176,14000  172,33000   94994135\n",
       "2010-01-06  158,20000        159.26  160,31000  154,39000   67031024"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203\n",
      "1988\n",
      "text    object\n",
      "dtype: object\n",
      "Open             object\n",
      "ClosingPrice    float64\n",
      "DailyHigh        object\n",
      "DailyLow         object\n",
      "VolumePcs         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(len(txts))\n",
    "print(len(prices))\n",
    "print(txts.dtypes)\n",
    "print(prices.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text    False\n",
      "dtype: bool\n",
      "Open             True\n",
      "ClosingPrice    False\n",
      "DailyHigh        True\n",
      "DailyLow         True\n",
      "VolumePcs       False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(txts.isnull().any())\n",
    "print(prices.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим вектор бинарных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized = []\n",
    "for i in np.arange(0,len(prices)-1):\n",
    "    if prices['ClosingPrice'].iloc[i+1] >= prices['ClosingPrice'].iloc[i]:\n",
    "        binarized.append(1)\n",
    "    else:\n",
    "        binarized.append(0)\n",
    "\n",
    "binarized.append(np.nan) #т.к. мы не знаем, как закрылись торги в день, следующий за последним в таблице\n",
    "prices['ClosingPrice_bin'] = binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClosingPrice</th>\n",
       "      <th>ClosingPrice_bin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>189.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>168.20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>175.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>159.26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>143.20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-09</th>\n",
       "      <td>161.79</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-10</th>\n",
       "      <td>160.54</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-11</th>\n",
       "      <td>169.89</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-12</th>\n",
       "      <td>182.49</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-13</th>\n",
       "      <td>189.30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ClosingPrice  ClosingPrice_bin\n",
       "Date                                      \n",
       "2010-01-02        189.85               0.0\n",
       "2010-01-03        168.20               1.0\n",
       "2010-01-04        175.00               0.0\n",
       "2010-01-06        159.26               0.0\n",
       "2010-01-07        143.20               1.0\n",
       "2010-01-09        161.79               0.0\n",
       "2010-01-10        160.54               1.0\n",
       "2010-01-11        169.89               1.0\n",
       "2010-01-12        182.49               1.0\n",
       "2010-01-13        189.30               1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prices = prices.drop(['Open', 'DailyHigh', 'DailyLow', 'VolumePcs'], axis=1)\n",
    "df_prices.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем пересечения между множеством дат новостей и аналогичным множеством дат торгов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n",
      "1159\n"
     ]
    }
   ],
   "source": [
    "df_prices = df_prices[df_prices.index.isin(idx)]\n",
    "txts = txts[txts.index.isin(idx)]\n",
    "print(len(df_prices))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ClosingPrice_bin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>\"Газпром\" не исключает в 2010 г. выпуска обли...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-19</th>\n",
       "      <td>\"Газпром\" готов забирать весь объем азербайдж...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-28</th>\n",
       "      <td>Консорциум во главе с российским ОАО \"Газпром...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-07</th>\n",
       "      <td>Газпромбанк открыл на Кипре дочернюю компанию...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-09</th>\n",
       "      <td>\"Газпром\" вновь понизил прогноз экспорта в Ев...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text  \\\n",
       "date                                                            \n",
       "2010-01-02   \"Газпром\" не исключает в 2010 г. выпуска обли...   \n",
       "2010-01-19   \"Газпром\" готов забирать весь объем азербайдж...   \n",
       "2010-01-28   Консорциум во главе с российским ОАО \"Газпром...   \n",
       "2010-02-07   Газпромбанк открыл на Кипре дочернюю компанию...   \n",
       "2010-02-09   \"Газпром\" вновь понизил прогноз экспорта в Ев...   \n",
       "\n",
       "            ClosingPrice_bin  \n",
       "date                          \n",
       "2010-01-02               0.0  \n",
       "2010-01-19               0.0  \n",
       "2010-01-28               1.0  \n",
       "2010-02-07               1.0  \n",
       "2010-02-09               1.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts['ClosingPrice_bin'] = df_prices['ClosingPrice_bin'].values\n",
    "txts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                False\n",
       "ClosingPrice_bin    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = txts[txts.index < pd.Timestamp(2016,1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = txts[txts.index >= pd.Timestamp(2016,1,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим два вектора - с и без применения tf-idf, чтобы проверить, дает ли улучшения использование tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tfdif_vect = TfidfVectorizer(lowercase=True, stop_words=None, use_idf=True, ngram_range=(1,1), smooth_idf=False)                        \n",
    "vector_model = tfdif_vect.fit(X_train['text'])\n",
    "tfidf_vector = vector_model.transform(X_train['text'])\n",
    "count_vector = count_vect.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tfidf_vector.toarray()\n",
    "x_train_c = count_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train['ClosingPrice_bin'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector_test = vector_model.transform(X_test['text'])\n",
    "count_vector_test = count_vect.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tfidf_vector_test.toarray()\n",
    "x_test_c = count_vector_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = X_test['ClosingPrice_bin'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель классификации. В качестве нее был выбран мультиномиальный наивный Байес."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58139534883720934"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23943661971830987"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.88      0.71       151\n",
      "        1.0       0.49      0.16      0.24       107\n",
      "\n",
      "avg / total       0.55      0.58      0.52       258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, дает ли улучшения использование tf-idf (ниже модель обучается на векторе, полученном без использования tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55426356589147285"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_c = MultinomialNB()\n",
    "clf_c.fit(x_train_c, y_train)\n",
    "predictions_c = clf_c.predict(x_test_c)\n",
    "accuracy_score(y_test, predictions_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, без tf-idf accuracy_score несколько ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем разные параметры альфа в модели наивного Байеса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0001 - accuracy: 0.5697674418604651\n",
      "Alpha: 0.001 - accuracy: 0.5813953488372093\n",
      "Alpha: 0.01 - accuracy: 0.5930232558139535\n",
      "Alpha: 0.1 - accuracy: 0.5697674418604651\n",
      "Alpha: 1 - accuracy: 0.5813953488372093\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "for a in alphas:\n",
    "    cl = MultinomialNB(alpha=a)\n",
    "    cl.fit(x_train, y_train)\n",
    "    preds = cl.predict(x_test)\n",
    "    print('Alpha: {} - accuracy: {}'.format(a,accuracy_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Творческая [4 балла]\n",
    "Придумайте и попытайтесь сделать еще что-нибудь, чтобы улучшить качество классификации. \n",
    "Направления развития:\n",
    "* Морфологический признаки: \n",
    "    * использовать в качестве признаков только существительные или только именованные сущности;\n",
    "* Модели скрытых тем:\n",
    "    * использовать в качестве признаков скрытые темы;\n",
    "    * использовать в качестве признаков динамические скрытые темы \n",
    "    пример тут: (https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/dtm_example.ipynb)\n",
    "* Синтаксические признаки:\n",
    "    * использовать SOV-тройки в качестве признаков\n",
    "    * кластеризовать SOV-тройки по усредненным эмбеддингам  (обученные word2vec модели можно скачать отсюда: (http://rusvectores.org/ru/models/ или https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md) и использовать только центроиды кластеров в качестве признаков\n",
    "* что-нибудь еще     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала попробуем использовать другие модели для классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "logregr = linear_model.LogisticRegression(penalty=\"l2\", fit_intercept=True, max_iter=100, C=1, solver=\"lbfgs\", random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55038759689922478"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logregr.fit(x_train, y_train)\n",
    "logregr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50387596899224807"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(x_train, y_train)\n",
    "knn.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, улучшений это не дало, наоборот - результат стал хуже. Попробуем использовать существительные и именованные сущности в качестве признаков.\n",
    "Для извлечения именованных сущностей используем библиотеку natasha *(можно установить командой pip install natasha)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natasha\n",
      "  Using cached natasha-0.9.0-py2.py3-none-any.whl\n",
      "Collecting yargy (from natasha)\n",
      "  Using cached yargy-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pymorphy2==0.8 in /Users/miron/anaconda/lib/python3.5/site-packages (from yargy->natasha)\n",
      "Collecting backports.functools-lru-cache==1.3 (from yargy->natasha)\n",
      "  Using cached backports.functools_lru_cache-1.3-py2.py3-none-any.whl\n",
      "Collecting intervaltree==2.1.0 (from yargy->natasha)\n",
      "  Using cached intervaltree-2.1.0.tar.gz\n",
      "Requirement already satisfied: docopt>=0.6 in /Users/miron/anaconda/lib/python3.5/site-packages (from pymorphy2==0.8->yargy->natasha)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /Users/miron/anaconda/lib/python3.5/site-packages (from pymorphy2==0.8->yargy->natasha)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /Users/miron/anaconda/lib/python3.5/site-packages (from pymorphy2==0.8->yargy->natasha)\n",
      "Requirement already satisfied: sortedcontainers in /Users/miron/anaconda/lib/python3.5/site-packages (from intervaltree==2.1.0->yargy->natasha)\n",
      "Building wheels for collected packages: intervaltree\n",
      "  Running setup.py bdist_wheel for intervaltree ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/miron/Library/Caches/pip/wheels/89/40/01/fa05b5a8202a472fb143815e7589fdf74369e710ca675cad11\n",
      "Successfully built intervaltree\n",
      "Installing collected packages: backports.functools-lru-cache, intervaltree, yargy, natasha\n",
      "Successfully installed backports.functools-lru-cache-1.3 intervaltree-2.1.0 natasha-0.9.0 yargy-0.10.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import NamesExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлечем все существительные (код взят из примера отсюда: https://stackoverflow.com/questions/33587667/extracting-all-nouns-from-a-text-file-using-nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/miron/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем извлечь все существительные из всех текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test if something is a noun\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "# do the nlp stuff\n",
    "tokenized = nltk.word_tokenize(str(all_text))\n",
    "nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "def get_nouns(file_text):\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = nltk.word_tokenize(file_text)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)]\n",
    "    nouns = [i for i in nouns if ( i not in string.punctuation )]\n",
    "    stop_words = stopwords.words('russian')\n",
    "    stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'на'])\n",
    "    nouns = [i for i in nouns if ( i not in stop_words )]\n",
    "    nouns = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in nouns]\n",
    " \n",
    "    return ' '.join(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы возьмем существительные из обучающих и тестовых данных отдельно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_test = []\n",
    "nouns_train = []\n",
    "\n",
    "for val in X_train['text'].values:\n",
    "    nouns_train.append(get_nouns(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in X_test['text'].values:\n",
    "    nouns_test.append(get_nouns(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С русским nltk работает не так хорошо пока что, но все равно попробуем обучить нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdif_vect = TfidfVectorizer(lowercase=True, stop_words=None, use_idf=True, ngram_range=(1,1), smooth_idf=False)                        \n",
    "vector_model = tfdif_vect.fit(nouns_train)\n",
    "tfidf_vector = vector_model.transform(nouns_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = count_vect.fit_transform(nouns_train)\n",
    "x_train = tfidf_vector.toarray()\n",
    "x_train_c = count_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector_test = vector_model.transform(nouns_test)\n",
    "count_vector_test = count_vect.transform(nouns_test)\n",
    "x_test = tfidf_vector_test.toarray()\n",
    "x_test_c = count_vector_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = MultinomialNB(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5736434108527132\n"
     ]
    }
   ],
   "source": [
    "cl.fit(x_train, y_train)\n",
    "preds = cl.predict(x_test)\n",
    "print('accuracy: {}'.format(accuracy_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5775193798449613\n"
     ]
    }
   ],
   "source": [
    "cl.fit(x_train_c, y_train)\n",
    "preds_c = cl.predict(x_test_c)\n",
    "print('accuracy: {}'.format(accuracy_score(y_test, preds_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = NamesExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_train = []\n",
    "names_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_names(text):\n",
    "    matches = extractor(text)\n",
    "    m = []\n",
    "    for match in matches:\n",
    "        start, stop = match.span\n",
    "        m.append(text[start:stop])\n",
    "    return ' '.join(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in X_train['text'].values:\n",
    "    names_train.append(extract_names(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in X_test['text'].values:\n",
    "    names_test.append(extract_names(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Алексей Миллер Миллер'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5658914728682171\n",
      "accuracy: 0.5775193798449613\n"
     ]
    }
   ],
   "source": [
    "tfdif_vect = TfidfVectorizer(lowercase=True, stop_words=None, use_idf=True, ngram_range=(1,1), smooth_idf=False)                        \n",
    "vector_model = tfdif_vect.fit(names_train)\n",
    "tfidf_vector = vector_model.transform(names_train)\n",
    "count_vector = count_vect.fit_transform(names_train)\n",
    "x_train = tfidf_vector.toarray()\n",
    "x_train_c = count_vector.toarray()\n",
    "tfidf_vector_test = vector_model.transform(names_test)\n",
    "count_vector_test = count_vect.transform(names_test)\n",
    "x_test = tfidf_vector_test.toarray()\n",
    "x_test_c = count_vector_test.toarray()\n",
    "cl = MultinomialNB(alpha=0.01)\n",
    "cl.fit(x_train, y_train)\n",
    "preds = cl.predict(x_test)\n",
    "print('accuracy: {}'.format(accuracy_score(y_test, preds)))\n",
    "cl.fit(x_train_c, y_train)\n",
    "preds_c = cl.predict(x_test_c)\n",
    "print('accuracy: {}'.format(accuracy_score(y_test, preds_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5697674418604651\n"
     ]
    }
   ],
   "source": [
    "tf_vect = TfidfVectorizer(lowercase=True, stop_words=None, use_idf=False, ngram_range=(1,1), smooth_idf=False)\n",
    "vector_model = tf_vect.fit(nouns_train)\n",
    "tf_vector = vector_model.transform(nouns_train)\n",
    "x_train = tf_vector.toarray()\n",
    "tf_vector_test = vector_model.transform(nouns_test)\n",
    "x_test = tf_vector_test.toarray()\n",
    "cl_1 = MultinomialNB(alpha=0.01)\n",
    "cl_1.fit(x_train, y_train)\n",
    "preds = cl_1.predict(x_test)\n",
    "print('accuracy: {}'.format(accuracy_score(y_test, preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
